---
id: 202602091836.replication.cosmos-policy-repro
title: Cosmos Policy 再現実験ガイド（環境・設備・手順）
type: replication
created_at: 2026-02-09T18:36:00+09:00
updated_at: 2026-02-09T18:36:00+09:00
status: draft
tags: [replication, robotics, cosmos-policy, libero, robocasa, aloha]
topic: Cosmos Policy Reproduction
sources:
  - arXiv:2601.16163
  - DOI:10.48550/arXiv.2601.16163
  - URL:https://github.com/NVlabs/cosmos-policy
  - URL:https://github.com/NVlabs/cosmos-policy/blob/main/README.md
  - URL:https://github.com/NVlabs/cosmos-policy/blob/main/SETUP.md
  - URL:https://github.com/NVlabs/cosmos-policy/blob/main/LIBERO.md
  - URL:https://github.com/NVlabs/cosmos-policy/blob/main/ROBOCASA.md
  - URL:https://github.com/NVlabs/cosmos-policy/blob/main/ALOHA.md
importance: high
timebox: "180m"
related:
  - 202602091832.paper_note.cosmos-policy
  - 202602091835.critique.cosmos-policy
owners: []
model: gpt-5
---

# 追試計画

## 目的/範囲
- 目的:
  - 公開コード/公開チェックポイントを用いて Cosmos Policy の主要結果を再現する。
  - 再現難易度が高い planning 系は「公開範囲で再現可能な部分」と「自前データで再構築が必要な部分」を分離する。
- 範囲:
  - Phase 1: LIBERO / RoboCasa / ALOHA の direct policy 再現。
  - Phase 2: ALOHA planning（公開 planning checkpoint 推論）。
  - Phase 3: 自前 rollout 収集による planning model 再学習。

## 環境/資材
- OS/実行基盤:
  - Linux x86-64。
  - Docker + NVIDIA Container Toolkit。
  - GPU ドライバは CUDA 対応版。
- 推奨ランタイム:
  - Python 3.10 系（docs の実行例準拠）。
  - PyTorch 2.7.0 + CUDA 12.8 系（paper再現時の近傍）。
  - `uv` による依存解決（`uv sync --extra cu128 --group <suite> --python 3.10`）。
- ハード要件（README記載）:
  - direct inference:
    - LIBERO: 1 GPU / 6.8GB VRAM 以上。
    - RoboCasa: 1 GPU / 8.9GB VRAM 以上。
    - ALOHA: 1 GPU / 6.0GB VRAM 以上。
  - planning inference (ALOHA):
    - 最小: 1 GPU / 10GB VRAM（直列）。
    - 推奨: N GPU / 各10GB VRAM（best-of-N 並列）。
  - 学習:
    - 最低目安: 1 node x 8 GPU (80GB級)。
    - 論文規模:
      - ALOHA: 8xH100 48h。
      - RoboCasa: 32xH100 48h。
      - LIBERO: 64xH100 48h。
- ソフト資材:
  - コード: `NVlabs/cosmos-policy`。
  - 事前学習済み/公開チェックポイント（HF）:
    - `nvidia/Cosmos-Policy-LIBERO-Predict2-2B`
    - `nvidia/Cosmos-Policy-RoboCasa-Predict2-2B`
    - `nvidia/Cosmos-Policy-ALOHA-Predict2-2B`
    - `nvidia/Cosmos-Policy-ALOHA-Planning-Model-Predict2-2B`（推論用）
  - データセット（任意/学習用）:
    - `nvidia/LIBERO-Cosmos-Policy`
    - `nvidia/RoboCasa-Cosmos-Policy`
    - `nvidia/ALOHA-Cosmos-Policy`
- 実機資材（ALOHA）:
  - ALOHA 双腕（ViperX 300 S x2） + 3カメラ（俯瞰1 + 手首2）。
  - policy server 用 GPU マシン + robot client 用制御マシン（同一でも可）。
  - 安全停止手段（E-stop）とオペレータ。

## 手順
- 0. 共通セットアップ:
  - `docker build -t cosmos-policy docker`
  - `docker run ... --gpus all --ipc=host ... cosmos-policy`
  - コンテナ内で対象 suite の依存導入。
  - `python cosmos_policy/verify_installation.py` で導入健全性チェック。
- 1. Phase 1（direct policy 再現）:
  - LIBERO:
    - `uv sync --extra cu128 --group libero --python 3.10`
    - `python -m cosmos_policy.experiments.robot.libero.run_libero_eval ...`
    - seed は `195/196/197`, `deterministic=True`。
  - RoboCasa:
    - `uv sync --extra cu128 --group robocasa --python 3.10`
    - `robocasa-cosmos-policy` を clone/install、asset script 実行。
    - `python -m cosmos_policy.experiments.robot.robocasa.run_robocasa_eval ...`
    - 同様に seed `195/196/197`。
  - ALOHA direct:
    - server: `python -m cosmos_policy.experiments.robot.aloha.deploy ...`
    - client: `python -m cosmos_policy.experiments.robot.aloha.run_aloha_eval ...`
    - 通信先は `http://<server_ip>:8777/act`。
- 2. Phase 2（planning 推論再現）:
  - `deploy.py` で planning model を指定。
  - 論文準拠に近い代表設定:
    - `num_queries_best_of_n=8`
    - `num_denoising_steps_action/future_state/value = 10/5/5`
    - future ensemble 3, value ensemble 5
    - `value_ensemble_aggregation_scheme=majority_mean`
    - `mask_current_state_action_for_value_prediction=True`
  - client 側で `return_all_query_results=True` を有効にして解析ログを残す。
- 3. Phase 3（planning model 再学習）:
  - まず direct policy で rollout データを収集（成功/失敗を含む）。
  - `cosmos_policy/config/experiment/cosmos_policy_experiment_configs.py` の rollout data path を自前データへ設定。
  - V(s') 版なら `mask_current_state_action_for_value_prediction=True` で学習。
  - 必要に応じ Q(s,a) 版（`mask_future_state_for_qvalue_prediction=True`）も別 run で比較。
- 4. ログ/成果物管理:
  - 評価ログは suite ごとに `.../logs/` 配下へ固定出力。
  - 重要メタ情報（git commit、CUDA、Torch、driver、seed、GPU型番）を run ごとに保存。

## 指標/受入基準
- direct policy（目安）:
  - LIBERO 平均 98.5 に対し ±1.0pt 以内。
  - RoboCasa 平均 67.1 に対し ±1.0pt 以内。
  - ALOHA 平均 93.6 に対し ±3.0pt 以内。
- planning（目安）:
  - 難2タスクで direct 比 +8pt 以上。
  - 論文報告 +12.5pt に近づくほど良い。
- 性能以外:
  - 1 rollout あたりの平均推論遅延（chunk単位）を計測。
  - 失敗タイプ（把持ミス、視認ミス、行動暴走）をラベル化。

## リスク/緩和
- 非公開 rollout 依存:
  - リスク: planning 学習結果が論文値に届かない。
  - 緩和: direct policy 再現を先に固定し、planning は改善幅評価に切替。
- 環境差分（Torch/CUDA/GPU）:
  - リスク: 成功率・安定性が変動。
  - 緩和: まず docs 推奨版へ固定、変更は1因子ずつ。
- 実機安全:
  - リスク: 長時間運用で把持失敗や衝突。
  - 緩和: 速度制限、E-stop 常備、危険姿勢を除外した初期条件で段階導入。
- 速度不足（planning）:
  - リスク: 実行遅延でタスク失敗。
  - 緩和: denoising step を減らした設定も並行評価（1/5/10）。

## 次アクション
- まず Phase 1（LIBERO/RoboCasa direct）を実施し、seed 3本の平均と分散を確定。
- 次に ALOHA direct を固定してから planning 推論を有効化。
- planning 再学習に進む場合は rollout 収集仕様（記録頻度・成功基準・初期条件分布）を先に文書化。

## References
- arXiv:2601.16163
- DOI:10.48550/arXiv.2601.16163
- URL:https://github.com/NVlabs/cosmos-policy
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/README.md
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/SETUP.md
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/LIBERO.md
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/ROBOCASA.md
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/ALOHA.md
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/cosmos_policy/experiments/robot/aloha/deploy.py
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/cosmos_policy/experiments/robot/aloha/run_aloha_eval.py
- URL:https://github.com/NVlabs/cosmos-policy/blob/main/cosmos_policy/config/experiment/cosmos_policy_experiment_configs.py
