# Recommend Papers and Follow-ups (2025-10-10)

## Next Reads (Physical Intelligence / VLA / World Models)

- FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution  
  - arXiv:2506.03173  
  - Physics-informed, multimodal world model for unbounded accretive surface growth. Introduces MAGE (Modality-Agnostic Growth Embedding) and Accretive Graph Network; provides SURF-GARDEN and SURF-BENCH for PI evaluation. Strong conceptual novelty with explicit Physical Intelligence framing.

- π0.5: a Vision-Language-Action Model with Open-World Generalization  
  - arXiv:2504.16054  
  - Extends π0 with co-training on heterogeneous data (multi-robot, high-level semantic prediction, web data). Learns hybrid multimodal examples (images, language, object detections, subtask predictions, low-level actions) for robust real-world generalization over long horizons.

- ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation  
  - arXiv:2505.22159  
  - Treats force sensing as a first-class modality in VLA via a force-aware MoE. Significant gains over strong π0-based baselines, especially under occlusion/dynamic contact.

- CoinRobot: Generalized End-to-end Robotic Learning for Physical Intelligence  
  - arXiv:2503.05316  
  - End-to-end framework explicitly targeting “Physical Intelligence,” integrating data, representation, and training pipeline.

- Toward General Physical Intelligence for Resilient Agile Manufacturing Automation (Review)  
  - arXiv:2508.11960  
  - Frames VLA within General Physical Intelligence (GPI) for industrial settings; synthesizes trends and readiness.

- A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots (Survey)  
  - arXiv:2506.20487  
  - Surveys Behavior Foundation Models (BFMs) for humanoid whole-body control; overlaps substantially with PI goals and future directions.

## Q&A on “Real-Time Execution of Action Chunking Flow Policies” and Related

### Q1: π0 vs. π0.5 — what’s different? structure? how do they differ from world models?
- Summary  
  - π0.5 (arXiv:2504.16054) builds on π0 and adds broad co-training: multiple robots, high-level semantic prediction, and web data. Uses hybrid multimodal examples combining images, language commands, object detections, subtask predictions, and low-level actions. Targets long-horizon generalization in real homes.  
  - π0 is the foundational VLA baseline broadly referenced by later works (e.g., ForceVLA, RTC contexts), typically with flow/diffusion action generation.
- Architectural differences (π0 → π0.5)  
  - Data diversity/co-training: π0.5 explicitly mixes heterogeneous sources for open-world generalization.  
  - Hybrid supervision: high-level semantics + low-level actions jointly learned, enabling long-horizon tasks.  
  - Execution compatibility: pairs well with chunked action execution (e.g., RTC/ACT) to handle latency and maintain smoothness.
- How they differ from “world models”  
  - VLA (π0/π0.5): conditionally generate actions directly from perception/language; emphasize policy generation and execution (often with flow/diffusion decoders and chunking).  
  - World models (e.g., Dreamer/PlaNet/GENIE/UniSim/FOLIAGE): explicitly learn dynamics (latent or pixel/video) for prediction/planning/rollouts; decisions often via MPC or actor-critic atop the model. Different design goal than directly decoding actions from multimodal prompts.

### Q2: Other world models and their architectures?
- Dreamer (V1/V2/V3)  
  - Latent recurrent state-space model (RSSM) for dynamics; actor-critic optimization from pixel observations; long-horizon latent rollouts.
- PlaNet  
  - Probabilistic latent dynamics + planning (CEM, etc.) from visual inputs; early representative.
- Video/world generative models  
  - GENIE/UniSim: large-scale video predictive models; can support planning or simulation-in-the-loop via learned dynamics.
- 3D/geometry-centric world models  
  - Implicit or occupancy scene models + dynamics for contact/geometry-rich tasks; combine perception and physics priors.
- FOLIAGE (arXiv:2506.03173)  
  - PI-oriented multimodal world model; unified context encoder across images/meshes/point clouds; physics-aware predictor conditioned on control; new latent (MAGE) with critic heads; emphasizes robustness (sensor dropout, long-horizon, modality transfer).

### Q3: With ~300 ms control delay allowed, can we offload control externally? follow-ups? VLA for swarms?
- Offloading and latency robustness  
  - Real-Time Chunking (RTC; arXiv:2506.07339) enables smooth asynchronous execution of chunked policies (freeze+inpaint), improving robustness to inference delay—well-suited for cloud inference of large VLA/flow/diffusion policies.  
  - FogROS2-PLR (arXiv:2410.05562), FogROS2-FT (arXiv:2412.05408): practical cloud robotics stacks focusing on probabilistic latency/reliability and fault tolerance; align well with remote inference pipelines.
  - Cloud-Assisted Remote Control for Aerial Robots (arXiv:2509.04095): theory→PoC for cloud-assisted control; offers concrete engineering insights for delay/robustness.
- “Big model offloading → small robot” follow-ups  
  - π0.5, ForceVLA 等は大規模VLAの実適用のボトルネック（汎化、接触）を直接改善。クラウド推論＋RTC/ACTの実行系と組みやすい。オープン実装系でもA100等クラウド推論は一般化。
- VLA for swarms  
  - 低レベル連続制御までVLA直制御するスウォーム事例は希少。現状はLLM×マルチロボ/スウォーム（SwarmChat, TACOS など）で高レベル協調・通信・割当・適応に注力。実務的には「オンボード低レベル安定化＋クラウド高レベル計画（チャンク/サブタスク）＋RTC/ACTで遅延吸収」のハイブリッドが現実解。

## References (arXiv IDs)
- ACT / low-cost bimanual IL: arXiv:2304.13705  
- Real-time chunking (flow/diffusion policies): arXiv:2506.07339  
- π0.5 (VLA, open-world): arXiv:2504.16054  
- ForceVLA (force-aware MoE): arXiv:2505.22159  
- FOLIAGE (PI world model): arXiv:2506.03173  
- FogROS2-PLR: arXiv:2410.05562; FogROS2-FT: arXiv:2412.05408  
- Cloud-Assisted Remote Control (UAV): arXiv:2509.04095  
- Behavior Foundation Model survey: arXiv:2506.20487  
- GPI review (manufacturing): arXiv:2508.11960  
- CoinRobot (PI end-to-end): arXiv:2503.05316  
