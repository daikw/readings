---
id: 202602091832.paper_note.cosmos-policy
title: Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning
type: paper_note
created_at: 2026-02-09T18:32:53+09:00
updated_at: 2026-02-09T18:32:53+09:00
status: draft
tags: [robotics, policy-learning, video-foundation-model, world-model, planning]
topic: Cosmos Policy
sources:
  - arXiv:2601.16163
  - DOI:10.48550/arXiv.2601.16163
  - URL:https://github.com/NVlabs/cosmos-policy
  - URL:https://research.nvidia.com/labs/dir/cosmos-policy
importance: high
timebox: "120m"
related: []
owners: []
model: gpt-5
---

# 単一論文ノート

## TL;DR
- Cosmos-Predict2-2B を「構造変更なし」でロボット方策に転用し、行動・未来状態・価値をすべて latent frame として同一拡散過程で生成する。
- 直接方策としても強く、LIBERO 98.5%、RoboCasa 67.1%、ALOHA 平均 93.6 を報告。
- rollout データで world model/value を再学習し、best-of-N 計画で難タスクに対して平均 +12.5pt 改善を示す。

## 背景/貢献
- 既存の video-policy 系は「多段学習」や「追加 action head」に依存しがちで複雑化しやすい。
- 本論文は latent diffusion sequence へモダリティを挿入するだけで、policy/world model/value を統合。
- 学習は 50/25/25 のバランスバッチ（policy/world/value）+ 補助目標で行い、単純 BC より性能を上げる設計。
- 推論時は direct policy と planning policy を切替可能。planning は dual deployment（policy model と planning model を分離）で運用。

## 方法/設定
- latent frame injection:
  - 例（11 frame）: blank, 現在proprio, wrist, primary, secondary, action, 未来proprio, 未来wrist, 未来primary, 未来secondary, value。
  - 低次元ベクトル（action/proprio/value）は [-1,1] 正規化し、latent volume 全体へ重複配置。
- 目的関数:
  - policy: p(a,s',V(s')|s)
  - world: p(s',V(s')|s,a)
  - value: p(V(s')|s,a,s')（rollout学習時は mask で V(s') または Q(s,a) へ切替）
- 計画:
  - best-of-N で action 候補を生成し、各候補に対する未来状態と価値を推定して最大価値を採用。
  - 論文設定は world 3回 × value 5回の 15 値を集約（majority mean）。
- 主要データ規模（論文）:
  - LIBERO: 500 demo/suite × 4 suites。
  - RoboCasa: 24 task、各50 demo（人手のみで比較）。
  - ALOHA: 4 task 合計185 demo。

## 結果/制約
- 主結果:
  - LIBERO 平均 98.5（OpenVLA-OFT 97.1, CogVLA 97.4）。
  - RoboCasa 平均 67.1（同等帯の FLARE/HAMLET 66.4、demo 数は Cosmos が 50/task）。
  - ALOHA 平均 93.6（π0.5 は 88.6）。
- planning:
  - 難しい2タスク（candies, ziploc）で +12.5pt（平均）改善。
  - V(s') ベース（model-based）が Q(s,a) ベース（model-free）より高性能。
- ablation:
  - 補助目標除去で -1.5pt、pretrain なしで -3.9pt（LIBERO）。
- 制約:
  - planning は遅い（論文本文で約5秒/chunk の言及）。
  - rollout 依存が強く、world/value 精度確保に追加データが要る。
  - 放出されたのはコード/重み/一部データで、ALOHA planning 用 648 rollout データ自体は未公開。

## 実装照合（論文↔コード）
- latent frame injection と index 管理:
  - `cosmos_policy/experiments/robot/cosmos_utils.py` の `get_action` で placeholder 画像列を構築し、action/proprio/value latent index を明示管理。
- value 集約（majority mean）:
  - `cosmos_policy/experiments/robot/cosmos_utils.py` の `aggregate_value_predictions` に実装。
  - success threshold=0.05、過半数が成功予測のときのみ成功群平均を返し、それ以外は 0。
- V(s') / Q(s,a) の mask 切替:
  - `cosmos_policy/models/policy_video2world_model.py` の `mask_current_state_action_for_value_prediction` と `mask_future_state_for_qvalue_prediction` で制御。
- 高σ重み付け（論文A.2.1）:
  - `cosmos_policy/modules/hybrid_edm_sde.py` で 70% log-normal + 30% uniform[1,85] を実装。
  - 推論用 config で `sigma_max=80, sigma_min=4` を使用（`cosmos_policy/config/experiment/cosmos_policy_experiment_configs.py`）。
- ALOHA planning 設定の痕跡:
  - `cosmos_policy/experiments/robot/aloha/run_aloha_eval.py` 冒頭コメントに「best-of-8 / 3 future ensemble / 5 value ensemble / majority mean / 10-5-5 steps」を明記。

## 気づき/疑問
- 本質は「モダリティを latent frame に埋めて同一生成器で学習」している点で、architecture scaling の恩恵をそのまま受けられる。
- ただし value 推定は rollout 分布に強く依存し、OOD 初期状態でのキャリブレーションは依然として不安定要因。
- majority mean は頑健だが、閾値 0.05 固定のためタスク別に過小/過大評価が起きうる。
- 実運用では receding horizon を切って full chunk 実行しているため、計算量は抑えやすい一方で中間修正性が弱い。

## 転用メモ（再現観点）
- 最初の再現は direct policy だけで行い、planning は後段に分離するほうが安全。
- 自前タスクへ移すときは以下を先に固定:
  - latent 順序（state_t, conditional frame 定義）
  - action/proprio/value 正規化レンジ
  - ノイズ分布（hybrid sigma）
  - chunk 実行長と再クエリ周期
- planning を使う場合、world/value 用 rollout を追加収集し、policy と planning model を分ける運用が再現しやすい。

## References
- arXiv:2601.16163
- DOI:10.48550/arXiv.2601.16163
- URL:https://github.com/NVlabs/cosmos-policy
- URL:https://research.nvidia.com/labs/dir/cosmos-policy
