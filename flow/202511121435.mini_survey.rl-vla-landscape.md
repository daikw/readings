---
id: 202511121435.mini_survey.rl-vla-landscape
title: VLA×RLの系譜と位置づけ — 2018–2025軽量サーベイ
type: mini_survey
created_at: 2025-11-12T14:35:00+09:00
updated_at: 2025-11-12T14:35:00+09:00
status: draft
tags: [VLA, RL, robotics, survey, flow]
topic: 視覚・言語・行動（VLA）と強化学習の統合アプローチ
sources: [
  "arXiv:1711.07280",  # R2R (VLN)
  "arXiv:1912.01734",  # ALFRED
  "arXiv:2204.01691",  # SayCan
  "arXiv:2212.06817",  # RT-1
  "arXiv:2307.15818",  # RT-2
  "arXiv:2205.06175",  # Gato
  "arXiv:2303.03378",  # PaLM-E
  "arXiv:2210.03079",  # VIMA
  "arXiv:2106.01345",  # Decision Transformer
  "arXiv:2410.24164",  # π0
  "arXiv:2510.25889"   # π_RL
]
importance: high
timebox: "60m"
related: [202511051058.paper_note.pi-rl-flow-vla, 202511121350.paper_note.gateflow-shortcut-vla]
owners: []
model: 
---

# 軽量サーベイ

## スコープ/基準
- 対象: VLA（視覚・言語・行動）とRLを統合する手法を2018–2025から抽出。
- 基準: 「V+L→A」方策をRLで最適化/評価/補助する代表系統（指示追従、価値フィルタ、事前学習VLA、オフラインRL、フロー/拡散のRL微調整）。
- 除外: 純SFTのみや、行動に未接続のVLM最適化は参考外。

## 各論文サマリ
- R2R (VLN) — 画像観測+言語指示でナビゲーションをRL最適化。VLA×RLの初期実証。arXiv:1711.07280
- ALFRED — 日常タスクの言語指示追従（模倣+RL）。VLAの家庭内操作ベンチマーク確立。arXiv:1912.01734
- SayCan — LLMの高レベル提案を、ロボ側のQ値（RL）で可行性フィルタし実行。計画×価値融合の典型。arXiv:2204.01691
- RT‑2 — Web‑VLM知識を継承したVLAを大規模SFT。直接RLでなくとも「V+L→A」を事前学習で獲得。arXiv:2307.15818
- RT‑1 — 実機操作データによるスケールSFT。後段でRL/選好と相補可能な基盤。arXiv:2212.06817
- Gato — 汎用系列モデリングでVLAを含む多様タスクを単一ポリシで表現。arXiv:2205.06175
- PaLM‑E — 言語・視覚・ロボ表現の統合トランスフォーマ。言語接地と行動生成の共通化。arXiv:2303.03378
- VIMA — マルチモーダルプロンプトで一般化把持/配置。VLAのプロンプト化の先駆。arXiv:2210.03079
- Decision Transformer — オフラインRLを系列最適化として定式化（VLA拡張の基礎）。arXiv:2106.01345
- π0 — Flow Matching系のVLA基盤。拡散/フロー世代のVLAに接続。arXiv:2410.24164
- π_RL — フロー型VLA（π0/π0.5）をオンラインRLで直接微調整。尤度扱い困難をFlow‑Noise/Flow‑SDEで解決。arXiv:2510.25889

## 比較/差分
- 早期系（VLN/ALFRED）: 言語条件付き方策を環境報酬で最適化（RL単体または模倣+RL）。
- 計画×価値（SayCan）: LLMの提案をRL価値で選別するモジュール分解。安全性/可行性に強い。
- 事前学習VLA（RT/Gato/PaLM‑E/VIMA）: 大規模SFTで「V+L→A」を獲得。RLは後段微調整や評価で併用。
- オフラインRL/系列（DT）: 実機データを安全に活用。オンラインと組み合わせて改善可能。
- フロー/拡散×RL（π0→π_RL）: 生成的方策の“尤度不可扱”に対し、RL適用のための確率過程設計（SDE/離散MDP）を導入。

## ギャップ
- スパース/言語報酬の頑健化（VLM/人間選好のバイアス・評価安定性）。
- 実機長期タスクでの安全探索と計算資源要件（並列化/遅延/ドメインギャップ）。
- フロー/拡散方策の理論整合（NLL近似、安定更新、探索分布の制御）。
- ベンチ整備（長期指示・複雑操作・スキル移送）と共通メトリクス。

## 次アクション
- π0系VLAに対するπ_RLの再現と、オフラインRL/DTによる初期安定化→オンライン微調整の二段戦略比較。
- SayCan流の価値フィルタをπ_RLに併用（高レベル可行性で探索を絞り、実機安全性を担保）。
- VIMA/RT‑2系データでの事前学習重みを活用したSim2Real転移と、GateFlow型のショートカット抑制の併用。
- VLM/人間選好を使った多視点評価（成功/安全/説明）でRL微調整の外部検証を設計。

## 図解（簡易スケッチ）
- R2R/ALFRED（言語条件付きRL）
  - [画像観測] + [言語指示] → [方策πθ] —(RL/模倣+RL)→ [離散行動]
- SayCan（LLM計画 × RL価値）
  - [LLM: 候補タスク/スキル] → [Q(s,a)で可行性スコア] → [実行]
- RT‑1/RT‑2（大規模SFT VLA）
  - [画像/言語] → [トランスフォーマ] —(SFT)→ [行動トークン/連続制御]（後段でRL可）
- Gato（汎用系列）
  - [マルチモーダル履歴] → [単一モデル] → [各種アクション]
- PaLM‑E（統合表現）
  - [視覚/言語/ロボ] → [統合表現Tr] → [行動]
- VIMA（マルチモーダルプロンプト）
  - [指示=プロンプト] + [視覚] → [方策] → [操作]
- Decision Transformer（オフラインRL）
  - データD={(s,a,r)} —(系列最適化)→ [予測a | 目標Return]
- π0（Flow系VLA）
  - [視覚/言語] → [Flow Matching生成] → [行動]
- π_RL（Flow系VLAのRL微調整）
  - [π0/π0.5] —(Flow‑Noise: 離散MDP, 厳密logπ)→ [RL更新]
  - [π0/π0.5] —(Flow‑SDE: 確率流で探索/相互作用統合)→ [RL更新]

## コンパクト比較表
| 系統 | 代表 | 学習信号 | RLの役割 | 行動表現 | 言語の役割 | 代表ベンチ | 備考 |
|---|---|---|---|---|---|---|---|
| 言語条件付きRL | R2R/ALFRED | 報酬/模倣+報酬 | 直接最適化 | 離散 | 指示条件 | R2R/ALFRED | 早期のVLA×RL |
| 計画×価値 | SayCan | SFT+価値推定 | 可行性フィルタ | スキル実行 | 高レベル計画 | 実機操作 | 安全/可行性に強い |
| 大規模SFT VLA | RT‑1/RT‑2/Gato/PaLM‑E/VIMA | 教師あり | 後段で補助 | 離散/連続/トークン | 指示/プロンプト | 実機/シム | 事前学習が主 |
| オフラインRL/系列 | Decision Transformer | オフライン | 直接/間接 | 離散/連続 | 文脈条件/Return条件 | 多様 | 安全・データ効率 |
| フロー/拡散×RL | π0→π_RL | SFT→オンラインRL | 直接最適化 | 生成（Flow） | 条件付け | LIBERO/ManiSkill | logπ扱いの解法提示 |

## 本サーベイにおける「π_RL」の位置づけ（明確化）
- カテゴリ: 「フロー/拡散VLAのエンドツーエンドRL微調整」。
- 新規性: Flow Matching系VLAの“log π(a|s)不可扱”問題に対し、
  - Flow‑Noise: 除去過程を離散MDP化し厳密対数尤度を導出→安定な方策更新を実現。
  - Flow‑SDE: 確率流SDEへ拡張し、探索と環境相互作用を同一過程で扱う設計→効率的探索。
- 貢献範囲: π0/π0.5を実用規模のオンラインRLで大幅改善（LIBERO/ManiSkill）。
- 位置づけ結論: 「RL+VLA」の初出ではないが、フロー型VLAに“RLを原理整合で適用する”最初期の代表実装であり、拡散/フロー系のRL適用における設計パターン（SDE化/二層MDP化）を確立。

## References
- R2R (VLN) — arXiv:1711.07280
- ALFRED — arXiv:1912.01734
- SayCan — arXiv:2204.01691
- RT‑1 — arXiv:2212.06817
- RT‑2 — arXiv:2307.15818
- Gato — arXiv:2205.06175
- PaLM‑E — arXiv:2303.03378
- VIMA — arXiv:2210.03079
- Decision Transformer — arXiv:2106.01345
- π0 — arXiv:2410.24164
- π_RL — arXiv:2510.25889
