---
id: 202602161237.paper_note.science-of-scaling-agent-systems
title: "Towards a Science of Scaling Agent Systems"
type: paper_note
created_at: 2026-02-16T12:37:48+09:00
updated_at: 2026-02-16T12:37:48+09:00
status: draft
tags: [ai-agents, scaling-laws, evaluation, engineering, planning]
topic: Agent systems scaling behavior
sources:
  - arXiv:2512.08296
  - DOI:10.48550/arXiv.2512.08296
  - URL:https://arxiv.org/abs/2512.08296
  - URL:https://arxiv.org/html/2512.08296v2
importance: high
timebox: "90m"
related: []
owners: []
model: gpt-5
---

# 単一論文ノート

## TL;DR
- エージェント性能は「LLMサイズを上げれば単調改善」ではなく、`agentic RL`（試行予算・探索）と `agentic engineering`（ツール構成・状態管理）で非線形に変わることを、4872実験で定量化。
- Train-to-test transfer は全体に弱く、ID強化はOODで悪化しやすい。追加計算はOODを伸ばすがIDを落とすケースもあり、配分設計が主要論点。
- 改善余地は依然大きく、最良構成でも対象ベンチマークの手作業上限比で約17.2%に留まる。

## 背景/貢献
- 既存の scaling law は主に pretraining/test-time compute に焦点があり、複数モジュールからなる agent systems の scaling を十分に説明できていない。
- 本研究は agent systems 向けに、性能を 4軸で分解して比較:
  - `L`: foundation LLM の能力
  - `R`: trial budget（反復・探索回数）
  - `E`: engineering quality（プロンプト・ツール・状態圧縮など）
- `D`: domain shift（ID/OODギャップ）
- 一貫プロトコルの下で、10ベンチマーク・11モデルを使った大規模実験を提示。

## 用語説明
- `agentic RL`: エージェントが複数試行や探索を通じて解を改善する運用側の強化。
- `agentic engineering`: ツール接続・状態管理・計画器など、システム設計で性能を上げる工夫。
- `ID`（in-distribution）: 学習/調整時と近い分布の評価条件。
- `OOD`（out-of-distribution）: 学習/調整時と異なる分布の評価条件。
- `PASS@k`: 最大 `k` 回の試行で少なくとも1回成功する確率。
- `test-time compute`: 推論時に使う追加計算資源（試行回数、探索深さ、並列実行など）。

## 方法/設定
- 対象:
  - SWE-Bench Verified, RE-Bench, Terminal-Bench, MLE-Bench。
- 規模:
  - 40種の独立エージェントシステム設計、4872実験、120万件超のロールアウト。
- 比較因子:
  - agentic RL（例: PASS@k、parallel trials）
  - agentic engineering（例: 検索・計画・状態管理の有無、長期文脈処理）
  - agentic scaling（モデルサイズ、test-time compute配分）
- 解析:
  - ID/OOD分離評価、R^2・MAE 等の予測適合、システム品質の分散寄与を算出。

## 結果/制約
- 主結果:
  - ベンチマーク全体でサイズ効果は不均一で、`R` と `E` が大きい改善を生む。
  - PlanCraft では小型モデルを高品質システムに載せると、巨大モデルの素朴設定を上回る。
  - IDで有効な設定がOODでは逆効果になる例を確認（ID最適化の過適合）。
- 重要な定量ポイント:
  - OOD予測では agentic RL 指標が優勢で、ID指標のみより高い説明力（本文の reported R^2 / MAE 改善）。
  - PlanCraft の engineered setting は non-engineered 比で性能劣化幅を大きく抑制（本文で 39%〜70% 劣化 vs 4%〜18% 劣化と報告）。
  - SOTAエージェントでも手作業基準に対して約17.2%で、未充足ギャップが大きい。
- 制約:
  - 対象は4ベンチ・特定タスク群で、他領域（例: マルチモーダル実機ロボティクス）への外挿は未検証。
  - engineering quality の定義は実装依存要素を含み、再現時に運用差が入りやすい。
  - 実験コストが高く、全設定の網羅は難しい。

## 気づき/疑問
- 実務では「モデル更新」より先に `R` と `E` の設計（探索予算、状態圧縮、失敗復帰）を詰める方が費用対効果が高い可能性。
- 評価設計は ID/OOD を必ず分離し、ID最適化を直接デプロイ判断に使わない運用が必要。
- test-time compute の追加は万能でなく、ID/OODトレードオフを管理するメタ方策が要る。
- 未解決点は、`L,R,E,D` の相互作用をタスク横断で予測可能な統一則へ落とし込めるか。

## 転用メモ（実装/運用）
- エージェント改善の優先順:
  - 1) `E`（観測/記憶/計画の設計）
  - 2) `R`（試行回数・並列戦略）
  - 3) `L`（モデル更新）
- 評価パイプライン:
  - ID/OODを分けて継続記録し、`PASS@k` と失敗カテゴリを同時に追跡。
- 予算配分:
  - 追加推論コストは「OOD改善1pt当たりコスト」で管理し、ID低下を許容閾値で制約。

## References
- arXiv:2512.08296
- DOI:10.48550/arXiv.2512.08296
- URL:https://arxiv.org/abs/2512.08296
- URL:https://arxiv.org/html/2512.08296v2
