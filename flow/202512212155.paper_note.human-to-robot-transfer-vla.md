---
id: 202512212155.paper_note.human-to-robot-transfer-vla
title: 人間動画からロボットへの転移がVLAで出現する条件
type: paper_note
created_at: 2025-12-21T21:55:49+09:00
updated_at: 2025-12-21T21:55:49+09:00
status: draft
tags: [robotics, vla, transfer, human-data, pretraining]
topic: 人間エゴ視点動画を用いたVLAの転移学習
sources:
  - https://www.physicalintelligence.company/download/human_to_robot.pdf
  - https://www.physicalintelligence.company/research/human_to_robot
importance: medium
timebox: "60m"
related: []
owners: []
model: 
---

# 単一論文ノート

## TL;DR
- 人間のエゴ視点動画をVLAにそのまま混ぜるだけでも、十分に多様なロボット事前学習があると人→ロボ転移が“出現”する。  
- 人間データの利得は事前学習多様性に強く依存し、表現の人/ロボ重なりが増えるほど転移性能が上がる。  
- 4つの一般化シナリオで性能が大幅改善し、ブログでは約2倍の改善と報告。

## 背景/貢献
- 人間動画は安価で多様だが、ロボットへの直接利用はドメインギャップが大きい。  
- 明示的なアライメントや模倣変換なしに、スケールで転移が出現するかを検証。  
- 単純な共訓練レシピで、人間データの転移が事前学習の多様性とともに強化されることを実証。  

## 方法/設定
- ベースはπ0.5 VLA。人間データを「別のエンボディメント」とみなし同一目的で共訓練。  
- 人間データ収集は頭部カメラ＋手首カメラ（任意）でエゴ視点。  
- 3D手部キーポイント（両手17点）と頭部カメラの6DoFを復元し、エンドエフェクタ軌道に対応付け。  
- 学習目標は高レベルのサブタスク予測と言語条件付き低レベル行動予測（離散FASTトークン＋連続フロー）。  
- 人間データは近傍ロボットタスクと50:50混合で微調整（π0.5 + ego）。  

## 結果/制約
- 一般化軸（scene/object/task）で改善: Spice 32→71%、Dresser 25→50%、Bussing 53→63%。  
- 卵の色分けタスクで57%→78%のソート精度、平均+4個の正解。  
- 事前学習の多様性が増えるほど人間データの利得が増大し、表現空間の人/ロボ重なりも増える。  
- 一部タスクでは人間データがターゲットロボット実データに劣る（Bussing）。  
- 人間の「グリッパ動作」は推定せず、ロボットデータ依存。  

## 気づき/疑問
- 人間データは「別ロボットのエンボディメント」と同等の転移効果に近い。  
- 事前学習の多様性が“人間データを吸収する能力”を引き出す点が示唆的。  
- 受動的日常動画の大量収集に対して、同様の転移出現がどの程度起きるか。  
- タスク依存のギャップ（Bussing）を埋める条件は、データ量か観測視点か。  

## References
- https://www.physicalintelligence.company/download/human_to_robot.pdf
- https://www.physicalintelligence.company/research/human_to_robot
