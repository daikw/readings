# ReasoningBank (arXiv:2509.25140) — Algorithm Notes and Robotics Adaptation

Status: abstract-based analysis (full PDF not parsed yet). Details below are grounded in the abstract and standard agent-memory design patterns; treat implementation suggestions as a concrete, actionable blueprint. Will refine with exact specs once full text is locally available.

## 1) What is ReasoningBank?

- Goal: let LLM agents learn from continuous task streams by abstracting “generalizable reasoning strategies” from both successful and failed experiences.
- Core idea: contrastive distillation of lessons from success vs. failure, then retrieval of those lessons at test time; integrate new learnings back after each task to self‑evolve.
- MaTTS (Memory‑aware Test‑Time Scaling): allocate more compute per task to generate abundant, diverse experiences that strengthen contrastive signals → synthesize higher‑quality memory → better guidance for the next rounds. Memory and scaling form a positive feedback loop.

## 2) Where does ReasoningBank live in an implementation?

ReasoningBank functions as a retrieval‑augmented “reasoning memory” subsystem that sits beside the agent’s planner/policy:

- Position in stack:
  - Above: task interface (prompts, environment context, tools/APIs)
  - Sidecar: ReasoningBank (memory store + retriever + synthesizer)
  - Below: policy/executor (planner/LLM/VLA, tool use, environment interaction)

- Interfaces:
  - Write path (post‑task): ingest trajectories + self‑judged outcomes → synthesize strategies → store/update memory.
  - Read path (pre/intra‑task): given current task context → retrieve top‑k relevant strategies → condition the agent’s planning/inference.

Practically, this is a module exposing:
  - `add_experiences(episodes)` → returns new/updated `strategies`
  - `retrieve_strategies(context, k)` → returns strategy snippets + metadata
  - `consolidate(updates)` → merges, dedups, and re-scores strategies

Reference placement in code (suggested):
- `agent/`
  - `planner.py` (calls retrieve before planning)
  - `executor.py` (reports outcomes, logs)
- `memory/reasoning_bank/`
  - `store.py` (CRUD, schema, embedding index)
  - `retriever.py` (query formulation, ANN search, scoring)
  - `synthesizer.py` (contrastive summarization, canonicalization)
  - `scheduler.py` (MaTTS: variant generation, compute budget allocation)

## 3) Likely pipeline (abstract → concrete blueprint)

1) Experience Collection
- Run N attempts per task (diverse decoding temps/seeds/tools/paths), logging observations, actions, rationales, tool outputs.
- Self‑judgment for success/failure via task metrics or heuristic checks.

2) Contrastive Strategy Synthesis (the heart of ReasoningBank)
- Input: sets of positive (success) and negative (failure) attempts for same/similar tasks.
- Method: contrastive summarization prompts to extract “what worked vs. what failed and why,” producing:
  - Strategy text (generalizable rule-of-thumb / reasoning pattern)
  - Preconditions & triggers (when to apply)
  - Anti‑patterns / pitfalls (to avoid)
  - Minimal exemplars (short schematic, not raw long traces)
  - Confidence/utility scores (from outcomes, diversity, recency)
- Output: canonicalized strategy records with embeddings for retrieval.

Prompt skeletons (blueprint):
- Contrastive lesson extraction:
  - System: "You extract generalizable strategies from multiple attempts. Avoid task-specific literals; parameterize thresholds and angles."
  - User: "Given SUCCESS attempts: … and FAILURE attempts: … Produce: (1) strategy, (2) preconditions, (3) pitfalls, (4) minimal exemplar, (5) parameters with ranges."
- Canonicalization:
  - "Rewrite to remove instance-specific IDs/coordinates; express quantities as ranges; name the strategy succinctly."

3) Memory Store (ReasoningBank proper)
- Storage: strategy documents with JSON schema + vector index (e.g., FAISS) keyed by context and task descriptors.
- Maintenance: deduplication, clustering, aging/decay, promotion of consistently helpful strategies.

4) Retrieval & Conditioning at Test Time
- Query formation from current context (task spec, environment cues).
- Retrieve top‑k strategies and inject into the agent:
  - Prompt preamble (lessons learned), or
  - Structured conditioning fields (constraints, subgoal hints), or
  - Policy/control tokens (for non‑text decoders, feed a strategy embedding side‑channel).

5) Integration (Post‑Task Update)
- Append new experiences, re‑run contrastive synthesis for deltas, re‑score strategies based on observed efficacy.

6) MaTTS: Memory‑aware Test‑Time Scaling
- Allocate extra compute per task to create more varied attempts (beam sampling, tool variants, decomposition variants), thus stronger positive/negative contrast.
- Periodically synthesize/update memory from this richer pool, reinforcing a virtuous cycle.

## 4) Minimal data schema (suggested)

```json
{
  "strategy_id": "uuid",
  "title": "Short, generalizable rule",
  "strategy_text": "If X, prefer Y; verify Z; then ...",
  "preconditions": ["signals", "task features"],
  "pitfalls": ["what to avoid and why"],
  "applicability": {"domains": ["web", "coding"], "confidence": 0.72},
  "support": {
    "success_refs": ["ep#..."],
    "contrast_refs": ["ep#... (fail)"],
    "metrics": {"wins": 12, "losses": 3}
  },
  "embedding": "vector",
  "last_updated": "ISO8601"
}
```

Retrieval scoring (suggested blend):
- `sim(context, embedding)` (cosine)
- recency decay `exp(-Δt/τ)`
- effectiveness prior `log((wins+α)/(losses+β))`
- diversity bonus to avoid near-duplicates

## 5) Robotics adaptation plan

Use ReasoningBank as a “strategy memory” bridging high‑level tasking and mid/low‑level execution (VLA/Flow/ACT/RTC):

- Stack integration
  - High‑level: task planner (LLM/VLA text head) retrieves strategies for subtasking, recovery heuristics, and contact handling.
  - Mid‑level: skills/options (grasp types, approach angles, speed profiles) are selected/parameterized by retrieved strategies.
  - Low‑level: stable control with chunked execution (ACT/RTC, diffusion/flow policy) runs on-board; cloud hosts heavy inference & memory ops.

- Episode logging (per attempt)
  - Inputs: vision (RGB/D), proprio, force/torque, language task, scene graph/detections.
  - Outputs: action chunks, success metric (task completion, contact force bounds, retries), failure codes.

- Self‑judgment
  - Success: task done within constraints (time, force thresholds, precision).
  - Failure classes: slip, jam, occlusion, misalignment, force overshoot, timeouts.

- Contrastive strategy synthesis for manipulation
  - From successes vs. failures on similar tasks/scenes, distill rules like:
    - “For translucent lid removal, approach at 20–30° tilt; reduce closing speed after initial contact; if force > X for Y ms, retract 5 mm and re‑align.”
  - Represent as concise textual strategies + parameter ranges; attach sensor feature signatures (e.g., force transients, visual keypoints) to preconditions.

- Retrieval & conditioning paths
  - VLA text prompt: prepend “lessons” as constraints/subgoal hints.
  - Policy conditioning: map retrieved strategy to a compact embedding concatenated to observation tokens; expose to flow/diffusion policy.
  - Controller parameters: set chunk length, impedance gains, retry budget from strategy metadata.

- Failure taxonomy (seed set)
  - Alignment: lateral/axial misalignment; angle error
  - Contact: force overshoot; chatter; slip; stick–slip
  - Perception: occlusion; false detection; low light; specular
  - Execution: timeouts; grasp loss; trajectory blockage
  - Recovery: ineffective backoff; repeated retries without re-alignment

- MaTTS for robotics
  - Allocate more compute per task via:
    - Multiple candidate subtask decompositions (graph search over options library).
    - Diverse sampling of action chunks (temps/seeds) in sim or shadow‑mode.
    - Tool/path variants (grasp types, approach angles) auto‑tuned.
  - Use these richer attempts to strengthen contrast in synthesis; periodically consolidate into ReasoningBank.

- Offloading & latency
  - Use FogROS2‑PLR/FT for cloud execution of retrieval/synthesis and heavy VLA; on‑board runs safety‑critical loops. Combine with RTC to hide 200–300 ms latency via asynchronous chunking (freeze + inpaint mechanics).

## 6) Pseudocode sketches

High‑level loop:
```
for task in stream:
  ctx = build_context(task, obs)
  strategies = ReasoningBank.retrieve_strategies(ctx, k=3)
  plan = agent.plan(ctx, strategies)
  outcome, traj = execute(plan)   # ACT/RTC for chunked smooth control
  ep = package_episode(ctx, plan, traj, outcome)
  ReasoningBank.add_experiences([ep])  # later: contrastive synthesis batch
```

Contrastive synthesis (batch):
```
pos = sample_success_episodes(similar_to=ctx)
neg = sample_failure_episodes(similar_to=ctx)
draft_strats = LLM_contrastive_summarize(pos, neg)
canon = canonicalize(draft_strats)         # de‑specific, parameterize
embed = encode(canon)
store.update_or_add(canon, embed)
```

MaTTS (per task):
```
attempts = []
for variant in diversify(task, budget=compute_budget):
  out, tr = try_variant(variant)
  attempts.append((out, tr))
ReasoningBank.add_experiences(postprocess(attempts))
if time_to_consolidate():
  run_contrastive_synthesis()

API note (call sites):
- `planner.plan(ctx, strategies)` merges high-level subgoals with constraints extracted from strategies.
- `executor.execute(plan)` respects controller parameter hints injected by ReasoningBank.
```

## 7) Evaluation suggestions (robotics)

- Ablations: no memory / raw trajectory memory / success‑only memory / ReasoningBank (contrastive).
- Metrics: task success率、再試行回数、接触力超過率、時間、滑らかさ（jerk/チャンク境界の安定性）、遅延耐性。
- 代表タスク: 差し込み、蓋開け、配線、衣類ピック＆プレース等の接触リッチ系 + 長期家事タスク（π0.5の家事系に対応）。

## 8) Implementation notes

- Storage: FAISS/ScaNN + JSON store（LiteDB/SQLite/Parquet）; on‑robotは最近傍のサブセットをキャッシュ。
- Schema hygiene: 固有名詞・座標を相対化/パラメタ化して汎化。
- Safety: 策略が低レベル制御を直接上書きしない（境界条件とゲイン等の上限）、フェイルセーフはオンボードで完結。
- Latency budget: retrieval < 30 ms（キャッシュ/前取り）、RTCで推論待ちを非同期吸収。
- Versioning: strategies carry `version` and `provenance` (dataset/run IDs) to enable rollback and auditing.

## 9) Takeaways

- ReasoningBankは「成功/失敗の対比から抽象化した推論ストラテジ」を中核とする記憶モジュール。
- ロボットでは、接触失敗の再発防止やサブタスク計画の安定化に効く。RTC/ACTと組み合わせ、クラウド推論下でも滑らかさと遅延耐性を両立できる。
