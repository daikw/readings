---
id: 202602271111.mini_survey.ai-coding-agent-trends-2026
title: AIコーディングエージェント論文トレンド（2025後半-2026）
type: mini_survey
created_at: 2026-02-27T11:11:01+09:00
updated_at: 2026-02-27T16:05:27+09:00
status: draft
tags: [ai-coding-agent, swe-bench, tts, context-file, post-training]
topic: 2025後半以降のAIコーディングエージェント研究潮流（テーマ別整理）
sources: [arXiv:2509.16941, arXiv:2511.12884, arXiv:2512.17419, arXiv:2601.04171, arXiv:2601.20404, arXiv:2601.22129, arXiv:2602.02361, arXiv:2602.03411, arXiv:2602.07900, arXiv:2602.11988, arXiv:2602.22124]
importance: high
timebox: "60m"
related: [202602271110.paper_note.agentsmd-framework-design]
owners: []
model: gpt-5-codex
---

# 軽量サーベイ

## スコープ/基準
- 対象期間: 2025-09-01 以降（2026-02-27 時点）。
- 対象テーマ: 評価基盤、推論時スケーリング（TTS）、学習パイプライン、コンテキストファイル運用。
- 選定基準: arXiv一次ソースで実験設定と結果が確認できるものを優先。

## 用語説明
- **SWE-bench Pro/++**: 従来SWE-benchより現実性・多言語性・規模を拡張した評価基盤。
- **TTS (Test-Time Scaling)**: 候補軌跡の並列探索や選別で推論時性能を向上させる手法。
- **Contextual Verifier**: 実行テスト以外の文脈情報でパッチ品質を判定する検証器。
- **Context File (`AGENTS.md` 等)**: リポジトリ固有のエージェント運用指示ファイル。
- **SFT (Supervised Fine-Tuning)**: ラベル付き軌跡でモデルを教師あり微調整する学習手法。
- **RL (Reinforcement Learning)**: 実行フィードバックを報酬にモデルを強化学習する手法。
- **Pass@1**: 1回生成でタスクを解決した割合。主要な解決率指標。
- **Best@K**: K回生成した中から最良のものを選んだときの解決率。TTS効果の測定に使う。Pass@1 ≤ Best@K ≤ Oracle@K の順で甘くなる。
- **Oracle@K**: K回生成のうち正解が1件でもあればカウントする上限値。Best@K の理想ケース。
- **Trajectory（軌跡）**: エージェントが課題を解くときの行動・観察・推論のステップ列。Replay や TTS の基本単位。
- **Rollout**: 軌跡を1本生成する試行のこと。K=16 rollouts は16本生成することを指す。
- **Scaffold（エージェント基盤）**: エージェントのループ制御・ツール呼び出し・プロンプト管理を担うフレームワーク層。モデル本体とは独立。
- **Rubric（評価ルーブリック）**: パッチを採点するための項目リスト。Agentic Rubrics 論文では4軸（File Change / Spec Alignment / Integrity / Runtime）で構成され、項目ごとに重要度を付与する。
- **SWE-bench Verified**: 人手で検証済みのSWE-benchサブセット（500件）。誤ラベルを除去してあり、精度比較の標準的な舞台になっている。

## 各論文サマリ

### 評価基盤の拡張
- `arXiv:2509.16941` **SWE-Bench Pro**（2025-09-21）— 長時間・多ファイル修正を含む高難度ベンチ（1,865問題、41リポジトリ）を導入。既存モデルのPass@1は低水準で、実務難度との差を可視化。
- `arXiv:2512.17419` **SWE-Bench++**（2025-12-19）— PRから再現可能タスクを自動生成するパイプラインを提案。11言語・11,133件規模で評価データ生成のスケーラビリティを向上。
- `arXiv:2602.02361` **SWE-Universe**（2026-02-02）— PR由来の検証可能環境を80万件超で大規模自動構築。学習・評価基盤の供給制約を緩和。

### 推論時スケーリング／検証効率
- `arXiv:2601.04171` **Agentic Rubrics**（2026-01-07）— テスト実行なしで文脈付きルーブリック検証を行い、並列TTS時の性能を改善。環境構築コストの削減余地を示す。
- `arXiv:2601.22129` **SWE-Replay**（2026-01-29）— 軌跡再利用型TTSで、naive探索比コスト最大17.4%削減かつ性能最大+3.8%を報告。
- `arXiv:2602.07900` **Agent-Generated Tests 再評価**（2026-02-08）— エージェントが生成する大量テストの寄与は限定的と報告。観測目的の軽量テストへ偏る傾向を示し、コスト最適化の再検討を促す。
- `arXiv:2602.22124` **SWE-Protégé**（2026-02-25）— 小型モデルが必要時のみ強モデルへ相談する「疎な協調」学習枠組みを提案。少数回の相談で性能向上。

### 学習パイプライン一体最適化
- `arXiv:2602.03411` **SWE-Master**（2026-02-03）— 軌跡合成→長期SFT→実行フィードバックRL→TTSを一体最適化。オープン系で高い解決率を報告し、再現可能な学習レシピを提示。

### Context File の効果検証
- `arXiv:2511.12884` **Agent READMEs**（2025-11-17）— 2,303件のcontext file分析。機能要件偏重で、性能・セキュリティ等の非機能指示が薄い実務実態を報告。
- `arXiv:2601.20404` **AGENTS.mdの効率影響**（2026-01-28）— 10リポジトリ・124PRの比較で、AGENTS.mdあり条件が時間/トークン効率で有利な傾向を報告。
- `arXiv:2602.11988` **Evaluating AGENTS.md**（2026-02-12）— 文脈ファイルが成功率を下げ、推論コストを20%以上増やすケースを報告。冗長指示が逆効果になることを実証。

## 比較/差分

| テーマ | 動向 |
|--------|------|
| 評価基盤 | `SWE-bench` 単体から `Pro`（長期・高難度）と `++`（多言語・大規模生成）へ重心移動 |
| スケーリング戦略 | brute-force探索より `Replay` や `Rubrics` による検証・探索の効率化が主流化しつつある |
| 学習側 | `SWE-Universe` + `SWE-Master` でデータ供給からRLまでを一気通貫で最適化する動きが顕在化 |
| Context file | `2601.20404`（改善）と `2602.11988`（悪化）で相反報告。設計品質・実験条件への依存が大きく未収束 |

## ギャップ
- **標準化不足**: 成功率以外（時間・トークン・環境構築コスト）の比較プロトコルが統一されていない。
- **一般化不足**: 単一ベンチ最適化が、社内モノレポやレガシー環境に移植できる保証が弱い。
- **Context file の設計規約未整備**: 最小要件・禁止指示・検証方法が定まっておらず、相反報告の原因の一つ。
- **コスト効率の再定義**: テスト生成量・実行検証省略可否が主要最適化対象になったが、実測データが少ない。

## 実運用指針
1. **評価セットを2層化**: `短期修正タスク` と `長期/多ファイルタスク` を分け、SWE-Bench Pro 相当の難度差を再現する。
2. **比較条件を固定**: `baseline` / `Replay型` / `Rubrics型` / `疎な協調型` を同一モデル基盤で比較する。
3. **指標を三本柱化**: `解決率` + `コスト(時間/トークン)` + `副作用(既存テスト劣化/差分肥大化)` を必須記録。
4. **Context fileをA/B化**: `なし` / `最小版` / `詳細版` の3条件で成功率とコストを同時評価する。
5. **失敗原因を分解**: `探索` / `検証` / `仕様解釈` / `実装修正` に分類し、次サイクルで改善対象を1カテゴリに絞る。

## 独自提案: Replay + Rubrics + Protégé の実務構成

論文知見を合成した3層アーキテクチャ案。括弧内はエビデンスの根拠論文。

- **候補生成層** — 複数軌跡を並列生成する。Replay（`2601.22129`）を使って途中ステップからの分岐再開とし、全軌跡をゼロから生成する naive 探索を避ける。分岐点の選択基準は「未訪問ファイルの探索余地」と「推論段落数による処理強度」。
- **低コスト選別層** — 全候補に実行テストを回す前に Rubrics（`2601.04171`）で採点して上位 K 件を絞る。評価軸は論文の4軸（File Change / Spec Alignment / Integrity / Runtime）を起点に、リポジトリ固有のルールを追記する。Rubrics のコストはテスト生成比で約40%安い（`$0.29` vs `$0.52`）とされており、候補数が多いほど効果が出る。
- **コスト最適化オプション** — 強モデルの費用が制約になる場合、Protégé（`2602.22124`）方式を検討する。小モデル（7B）が大半の推論を担い、詰まったときのみ強モデルに委譲する。論文では1タスクあたりの強モデル呼び出しが平均4回（75ステップ中）、コストは強モデル単独の約1/8と報告されている。

## 独自提案: 導入設定案（初期値）

- **フェーズ1**（2週間）: 既存の single-shot を baseline として計測。解決率・時間・トークンの三本柱を記録する。
- **フェーズ2**（2〜4週間）: Replay を追加し `再利用あり/なし` を同一モデルで比較する。論文の目安（コスト−17%・性能+4%）を達成できるか確認する。
- **フェーズ3**（4週間目以降）: Rubrics 選別を追加し `全候補テスト` vs `Rubrics上位のみテスト` の品質差を確認する。候補数は `N=5`、実テスト対象は `K=2` から始める。
- **停止条件**: 回帰増加または解決率が baseline 比 −5pt 超の場合、Rubrics の閾値か Replay 分岐ルールを見直す。

## 次アクション
- 社内データで `Replay型` と `Rubrics型` のコスト効率を比較し、推論費の削減余地を計測する。
- `AGENTS.md` は最小指示テンプレートから開始し、A/Bで効果確認後にのみ項目を増やす。
- 2025後半以降のベンチ拡張（`Pro`, `++`）に対応した社内評価セット設計へ移行する。

## References
- [arXiv:2509.16941](https://arxiv.org/abs/2509.16941)
- [arXiv:2511.12884](https://arxiv.org/abs/2511.12884)
- [arXiv:2512.17419](https://arxiv.org/abs/2512.17419)
- [arXiv:2601.04171](https://arxiv.org/abs/2601.04171)
- [arXiv:2601.20404](https://arxiv.org/abs/2601.20404)
- [arXiv:2601.22129](https://arxiv.org/abs/2601.22129)
- [arXiv:2602.02361](https://arxiv.org/abs/2602.02361)
- [arXiv:2602.03411](https://arxiv.org/abs/2602.03411)
- [arXiv:2602.07900](https://arxiv.org/abs/2602.07900)
- [arXiv:2602.11988](https://arxiv.org/abs/2602.11988)
- [arXiv:2602.22124](https://arxiv.org/abs/2602.22124)
