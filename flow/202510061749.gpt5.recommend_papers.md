# 次に読むと面白そうな論文

## 調査概要

指定された論文 2 件はどちらもロボットの **バイマニュアル操作（両手操作）**に関するものです。2023 年の論文では低コストハードウェア（ALOHA）と模倣学習によりケーブルタイの締結や電池の挿入などの微細作業を実現し、その際に **Action Chunking with Transformers (ACT)** という生成モデルを導入しました [oai_citation:0‡arxiv.org](https://arxiv.org/abs/2304.13705#:~:text=,website%3A%20%2014%20this%20https)。2025 年の論文ではこの ACT ポリシーをリアルタイムに実行するための **Real‑Time Chunking (RTC)** アルゴリズムを提案し、拡散・フローに基づくビジョン言語行動モデルの遅延を克服しました [oai_citation:1‡arxiv.org](https://arxiv.org/abs/2506.07339#:~:text=world%2C%20increasingly%20require%20real,in%20the%20presence%20of%20significant)。これらの技術をベースとして、最近発表された関連研究を調べました。以下の論文は、それぞれ ACT/RTC の課題を拡張したり新しい方向性を示したりしており、次に読む論文として適していると考えられます。

## おすすめ論文

### InterACT: Inter‑dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation (CoRL 2024)

- **概要**: バイマニュアル操作における左右腕の相互依存性を明示的に考慮した模倣学習フレームワーク **InterACT** を提案。多モーダル入力を処理する **階層型アテンションエンコーダ** と、各腕の出力を共有しながら並列にアクションを生成する **マルチアームデコーダ** で構成される [oai_citation:2‡arxiv.org](https://arxiv.org/abs/2409.07914#:~:text=,validate%20the%20significance%20of%20key)。入力に対してセグメント別およびセグメント間の注意機構を用いて情報を集約し、各腕の中間出力を相手側の文脈として利用することで、既存手法より高い成功率を示した [oai_citation:3‡arxiv.org](https://arxiv.org/abs/2409.07914#:~:text=,validate%20the%20significance%20of%20key)。
- **なぜ面白いか**: ACT ベースの手法をさらに発展させ、両腕間の相互作用をモデル化する点が特徴。RTC と組み合わせればリアルタイム制御の精度向上が期待できる。

### Learning Bimanual Manipulation via Action Chunking and Inter‑Arm Coordination with Transformers (AIST, 2024)

- **概要**: ALOHA ロボットを用いた模倣学習のアーキテクチャを改良し、左右の腕にそれぞれ専用のエンコーダを用意しつつ、**Inter‑Arm Coordinated Transformer Encoder (IACE)** で腕同士の同期をとる新モデルを提案 [oai_citation:4‡arxiv.org](https://arxiv.org/html/2503.13916v1#:~:text=We%20propose%20a%20novel%20architecture%2C,split%20decoders%20and%20single%20decoders)。デコーダは単一または分割型の 2 種類を検証し、IACE なしのベースラインより 8–9 % 成功率が向上した [oai_citation:5‡arxiv.org](https://arxiv.org/html/2503.13916v1#:~:text=We%20conduct%20distinctive%20bimanual%20tasks,1)。
- **なぜ面白いか**: ACT の構造自体を改善しており、両腕の時間的同期を保ちながら高精度なアクションチャンクを生成する点が新しい。現在の ACT や RTC と比較・統合することで性能向上が望める。

### RDT‑1B: a Diffusion Foundation Model for Bimanual Manipulation (ICLR 2025 採択論文)

- **概要**: 両手操作のための大規模基盤モデル **Robotics Diffusion Transformer (RDT)** を提案。拡散モデルに基づき、複数ロボット間の異質なアクション空間を統一する **Physically Interpretable Unified Action Space** を導入 [oai_citation:6‡arxiv.org](https://arxiv.org/abs/2410.07864#:~:text=,dataset%20with%20over%206K%2B%20episodes)。1.2 B パラメータのモデルを 46 種類のロボット・環境データで事前学習し、6 K 以上のバイマニュアルエピソードで微調整している。これにより言語指示に従った操作、少数ショット学習、未見対象へのゼロショット一般化などを実現した [oai_citation:7‡arxiv.org](https://arxiv.org/abs/2410.07864#:~:text=original%20actions%2C%20facilitating%20learning%20transferrable,21%20this%20https%20URL%20rdt)。公式ページではロボット犬を操縦するデクスタラス操作や未知物体へのゼロショット適応などの動画を公開しており


# π₀とπ₀.5の違い・構造と他のワールドモデル比較

## Physical Intelligenceのπ₀とπ₀.5の違い

| モデル                                   | 構造の特徴                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 他モデルとの違い                                                                                                                                                                                                                                    |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **π₀ (Physical Intelligence社, 2024)**   | 事前学習された視覚-言語モデル（VLM）に連続動作出力の頭部を付加し、クロスエンボディメントなデータセットで教師あり学習される3B規模のVLAモデル。画像と自然言語のトークン列の後に“フロー・マッチング”法で連続的な行動トークンを生成する [oai_citation:28‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0#:~:text=Beyond%20training%20on%20many%20different,time%20dexterous%20robot%20control)。このアクション専門家により、ミリ秒単位で高周波の関節/エンドエフェクタ軌道が生成できる。                                                                                                                                                                                                                                                                                                                                            | 単一システム構成で、高レベル推論と低レベル制御が統合されているため推論は速いが、長期的なタスク分解や開放世界への一般化は弱い。                                                                                                                      |
| **π₀.5 (Physical Intelligence社, 2025)** | π₀を基に、**階層的二段推論**を導入したモデル。1) 視覚・言語の入力から「次にやるべきサブタスク（ハイレベル意味表現）」を予測する高レベルセマンティックプランナー、2) サブタスクと観測状態から連続動作トークンを生成する**アクション専門家**という二段構成。各段はTransformerに基づくが、タスク種別によって異なるトークン化や学習損失を利用 [oai_citation:29‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf)。π₀.5はWebデータ、複数のロボットによるデモ、タスク予測・ナビゲーションデータなど異種データを共同学習し、**未知の家庭環境でキッチンを片付けるといった15分規模の長いタスクをこなす** [oai_citation:30‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=behavior%20that%20is%20appropriate%20to,human%20%E2%80%9Csupervisors%E2%80%9D%20that%20walk%20the)。 | 階層的推論により長期タスクを小さなサブタスクに分解し、部分的失敗に対して再計画できる。異種データの共同学習により新しい物体や環境への一般化が向上。一方でモデルが大きく推論遅延が大きい（本論文のRTCアルゴリズムはこの遅延を非同期処理で緩和する）。 |

### π₀/π₀.5と他モデルの違い

* **アーキテクチャ**: π₀は単一システムによるエンドツーエンドの行動生成で、手先座標やスキルライブラリを持たない。π₀.5は高レベルサブタスク予測と低レベルアクション生成を分離し、長期タスクを実行する。PaLM‑E、RT‑2、OpenVLA等の他モデルはテキスト出力を介して低レベルコントローラに指示したり、行動トークンを離散化して表現する点が異なる（後述）。
* **データ利用**: π₀はロボットデモからの教師あり学習が中心だが、π₀.5はロボットデータに加えWeb動画から得たサブタスクデータやシミュレーションでの高レベルナビゲーションデータを併用 [oai_citation:31‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=behavior%20that%20is%20appropriate%20to,human%20%E2%80%9Csupervisors%E2%80%9D%20that%20walk%20the)。この共同学習により、未知の家庭で料理や掃除を実行する能力が向上した。
* **制御粒度**: どちらも連続動作を直接生成するが、π₀.5は50ステップ程度のアクションチャンクをまとめて生成し、高レベルプランニングとの同期を図る。π₀.5で導入された**Real‑Time Chunking (RTC)**アルゴリズムでは次のチャンクを推論している間に現在のチャンクを実行することで推論遅延を吸収する。

## 他のワールドモデルとアーキテクチャ

ワールドモデルは「環境の物理状態を内部で予測し、将来の状態を想像しながら計画・行動するモデル」である。VLA分野でもワールドモデルとの統合が進んでいる。

| モデル                             | アーキテクチャの特徴                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 注目点                                                                         |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| **FOLIAGE (2025)**                 | 物理インテリジェンスを「部分的な感覚情報から物理世界を推論する能力」と捉え、マルチモーダルワールドモデルを提案 [oai_citation:32‡ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2506.03173#:~:text=1%20Introduction)。**統一コンテキストエンコーダ**で画像・メッシュ・ポイントクラウドを共通潜在空間へ埋め込み、**物理に基づく予測器**が摩擦や成長など物理ダイナミクスを予測する [oai_citation:33‡ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2506.03173#:~:text=Surf,predictive%20accuracy%2C%20expressiveness%2C%20and%20robustness)。                                                                                          | 表面成長の長期予測を行い、ワールドモデルの重要性を強調。                       |
| **WorldVLA (Jun 2025)**            | VLAモデルとワールドモデルを統合した自回帰型アクション世界モデル。画像・テキスト・アクションを共有語彙でトークン化し、ワールドモデルが入力された行動に基づいて未来の画像を生成し、アクションモデルが画像から次の行動を生成する [oai_citation:34‡arxiv.org](https://arxiv.org/pdf/2506.21539.pdf#:~:text=To%20address%20the%20constraints%20inherent,The%20world%20model)。互いの出力が性能を向上させる。長いアクション列では誤差伝搬が顕著なため、**アクション注意マスク**を導入し先行アクションへの依存を減らすことでアクションチャンク生成を改善 [oai_citation:35‡arxiv.org](https://arxiv.org/pdf/2506.21539.pdf#:~:text=generation,Concurrently)。 | ワールドモデルとアクションモデルの相互強化を示し、世界モデル統合の流れを牽引。 |
| **World4Omni (Jun 2025)**          | 事前学習済みのマルチモーダル画像生成モデルをワールドモデルとして利用し、与えられたサブタスクから**未来の画像（サブゴール）**を予測する。推論されたサブゴール画像を点群に変換し、低レベルポリシーが操作を実行する [oai_citation:36‡arxiv.org](https://arxiv.org/pdf/2506.23919.pdf#:~:text=World4Omni%20Figure%201,position%20to%20its%20target%20position)。この方法により追加データ収集無しで多様な操作タスクをゼロショット実行できる [oai_citation:37‡arxiv.org](https://arxiv.org/pdf/2506.23919.pdf#:~:text=Coupled%20with%20zero,tuning.%20Sup)。                                                                                                | 画像生成モデルの強力な一般化能力を活かし、高レベル世界モデルとして利用。       |
| **3D‑VLA (ICML 2024)**             | 3D空間認識・推論・行動を統合するため、3DベースLLMにアクショントークンを付加し、**拡散生成モデル**でゴール画像と点群を予測する [oai_citation:38‡proceedings.mlr.press](https://proceedings.mlr.press/v235/zhen24a.html#:~:text=Recent%20vision,VLA%2C%20we%20curate%20a)。3D関連データセットを構築して学習し、3D空間での推論や計画を大幅に向上 [oai_citation:39‡proceedings.mlr.press](https://proceedings.mlr.press/v235/zhen24a.html#:~:text=this%20end%2C%20we%20propose%203D,related)。                                                                                                                                                            | 2DベースVLAの限界を指摘し、3D世界モデルと拡散生成による未来予測で精度向上。    |
| **V‑JEPA 2‑AC (ICLR 2025)**        | 大規模自己教師ありビデオエンコーダ上に行動条件付きの世界モデルを構築し、目標画像から未来の潜在状態をシミュレートしながら計画する [oai_citation:40‡arxiv.org](https://arxiv.org/html/2508.13073v1#:~:text=future%20goal%20images%20and%20point,shot%20robotic%20manipulation)。生成された目標をもとに低レベルポリシーがタスクを実行。                                                                                                                                                                                                                                                                                                                  | ビデオから学習した潜在表現を活用し、ゼロショット操作を実現。                   |
| **3D-VLAに続く他のワールドモデル** | RIGVidは拡散型世界モデルでタスク動画を生成し、VLMでフィルタリング後にポーズを抽出する [oai_citation:41‡arxiv.org](https://arxiv.org/html/2508.13073v1#:~:text=future%20goal%20images%20and%20point,video%20encoder%20trained%20on%20internet)。FoundationPoseはグリッパのポーズ抽出モジュールで、生成動画から手先軌跡を復元する。                                                                                                                                                                                                                                                                                                                     | 多段処理による高精度な動作生成。                                               |

## その他のVLAモデル・小型モデル

* **PaLM‑E**は大規模言語モデルにセンサデータをトークンとして注入し、高レベルプランをテキストとして生成する。「テキスト→ロボット行動」を別ポリシーに依存するため、VLAよりも役割が異なる [oai_citation:42‡research.google](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/#:~:text=Today%20we%20introduce%20PaLM,only%20task%20capabilities)。
* **RT‑2**は視覚と言語のモデルをロボット軌跡データとネットスケールの画像文データで同時微調整し、ロボット行動を離散トークンとして出力する [oai_citation:43‡robotics-transformer2.github.io](https://robotics-transformer2.github.io/assets/rt2.pdf)。チェーンオブソートを利用して長いタスクを推論。
* **OpenVLA 7B**はLLama 2/DINOv2ベースのオープンソースVLAで、約97万エピソードのロボットデータと視覚文データで事前学習。微調整が容易で、多くの研究のベースラインになっている [oai_citation:44‡arxiv.org](https://arxiv.org/pdf/2406.09246.pdf#:~:text=Large%20policies%20pretrained%20on%20a,OpenVLA%20for%20new%20settings%2C%20with)。
* **Helix**はFigure AIによる二系統構造：7B VLMを用いた高レベル理解（7‑9 Hz）と、200 Hzで動作する高速ビジューモータトランスフォーマを分離し、協調しながら運転する [oai_citation:45‡figure.ai](https://www.figure.ai/news/helix#:~:text=%2A%20Full,torso%2C%20head%2C%20and%20individual%20fingers)。複数ロボットの協調や全身制御に成功している [oai_citation:46‡figure.ai](https://www.figure.ai/news/helix#:~:text=Helix%20is%20a%20first,the%20entire%20humanoid%20upper%20body)。
* **SmolVLA**は小型モデルと非同期推論スタックにより、単一GPUでの学習と高速推論を実現 [oai_citation:47‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=Vision,its%20compact%20size%2C%20SmolVLA%20achieves)。リアルタイム推論とエッジデプロイメントに適している。
* **LeVERB**はヒューマノイドの全身制御にVLAを適用するため、視覚言語方針と強化学習で学習したWBCポリシーを階層的に組み合わせる [oai_citation:48‡arxiv.org](https://arxiv.org/html/2506.13751v1#:~:text=WBC%2C%20the%20first%20of%20its,8%20times)。人間の「速い反射と遅い思考」を模した二重過程アーキテクチャを導入 [oai_citation:49‡arxiv.org](https://arxiv.org/html/2506.13751v1#:~:text=Humans%20think%20both%20fast%20and,rich%20vision%20and%20language%20inputs).
* **Surveyの提案** – VLAモデルの体系的な調査では、モノリシック・デュアルシステム・階層モデルの区別を整理し、将来の方向性としてメモリ機構、4D知覚、効率的な適応、**マルチエージェント協調**を挙げている [oai_citation:50‡arxiv.org](https://arxiv.org/html/2508.13073v1#:~:text=model%20integration%3B%20,This%20survey%20consolidates%20recent)。

## オフロード推論・ロボットスウォーム研究

### クラウドやエッジへのオフロード

* **Real‑Time Execution of Action Chunking Flow Policies (RTC)** – 本論文はVLAモデルの推論遅延に着目し、現行チャンクを実行しながら次のチャンクを生成する**非同期アルゴリズム**を導入して300 ms程度の遅延を吸収する。この仕組みによりクラウド上でVLAを推論しつつ現場で連続行動を出力できる。
* **FogROS2‑PLR** – モバイルロボットを複数のネットワークとクラウドサーバに接続し、期限内に結果を受け取る確率を最大化するルーティング機構を備えたクラウドロボティクス基盤。均一なWi‑Fi/5G環境でも平均遅延を36 %短縮し、P99遅延を最大3.7倍向上 [oai_citation:51‡arxiv.org](https://arxiv.org/html/2410.05562v1)。重い認識モデルやVLAをクラウドにオフロードする際の基盤として利用できる。
* **SmolVLA/LoHoVLA** – 小型モデルと非同期推論によってオンボードのGPUで実行できるよう設計された研究。低クロックのロボット制御に特化しており、クラウドへの依存を減らせる [oai_citation:52‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=Vision,its%20compact%20size%2C%20SmolVLA%20achieves).
* **Knowledge Insulation/FASTトークナイズ** – π₀.₅系の後続研究では、推論速度を高めるためモダリティトークンをフーリエ変換して1/4に圧縮する方法や、古い知識の劣化を防ぐKnowledge Insulationを提案している。このような技術によりモデルの推論遅延を短縮し、外部オフロードやエッジ推論に適した構造が検討されている（Physical Intelligence社ブログ記事より）。

### ロボットスウォーム・マルチエージェント

* **マルチエージェント協調は未開拓領域** – 2025年のVLAサーベイでは「多エージェント協調」を重要な将来課題として挙げており、重量物の運搬や協働工具使用などでエージェント間の意図共有・役割割り当てが必要になると指摘している [oai_citation:53‡arxiv.org](https://arxiv.org/html/2508.13073v1#:~:text=Multi,of%20addressing%20complex%20group%20objectives)。現時点では単一ロボットに特化したVLAが中心であり、複数ロボットを同時に制御する汎用VLAは報告されていない。
* **MALMM** – Multi‑Agent Large Language Models for robotic manipulationは複数のLLMエージェント（プランナー・コーダー・スーパーバイザ）を組み合わせて長期タスクを遂行するフレームワークで、再計画と誤り回復を実現している [oai_citation:54‡arxiv.org](https://arxiv.org/html/2411.17636v2#:~:text=Large%20Language%20Models%20,horizon%20manipulation%20tasks%2C%20and)。しかしここでの“マルチエージェント”は物理ロボットではなくモデル内部の役割分担であり、ロボット群を同時に制御するものではない。
* **Helixの協調** – Helixは複数のヒューマノイドが協働作業を行うデモを示しており、低速なプランニングモデルと高速な反応モデルの二層構造でマルチロボット協調を可能にしている [oai_citation:55‡figure.ai](https://www.figure.ai/news/helix#:~:text=%2A%20Full,torso%2C%20head%2C%20and%20individual%20fingers) [oai_citation:56‡figure.ai](https://www.figure.ai/news/helix#:~:text=Helix%20is%20a%20first,the%20entire%20humanoid%20upper%20body)。ただし組み合わせは数台のロボットに限定され、スウォーム（多数ロボット）の自律分散制御ではない。

## まとめと展望

* **π₀とπ₀.5** – どちらも大規模VLMを基盤に連続動作を直接生成するが、π₀.5は高レベル・低レベルの二段推論を導入し、異種データの共同学習によって開放世界への一般化を向上した。RTCアルゴリズムやトークナイズ高速化など推論遅延を減らす技術が盛り込まれている。
* **ワールドモデル統合** – FOLIAGE, WorldVLA, World4Omni, 3D‑VLAなどは物理ダイナミクスを内生的に理解するためのワールドモデルを提案し、未来の画像や点群を生成して低レベルポリシーを誘導する。これらのモデルは長期計画や未知環境への適応に強みを示しており、π₀系でも今後の発展が期待される。
* **小型モデルとクラウドオフロード** – SmolVLAやFogROS2‑PLRの研究は、制御遅延が300 ms程度で許容される場合にクラウドやエッジに推論をオフロードしたり、小型モデルでオンボード実行する方法を模索している。π₀.5のRTCやFASTトークナイズもオフロードを前提とした高速化策と解釈できる。
* **ロボットスウォームへの応用は未到達** – 現状のVLA研究は単一ロボットまたは少数の協調に焦点を当てており、ドローン群や数十台規模のロボットスウォームをVLAで制御する報告は見当たらない。サーベイでは将来の課題としてマルチエージェント協調や記憶機構を挙げており [oai_citation:57‡arxiv.org](https://arxiv.org/html/2508.13073v1#:~:text=Multi,of%20addressing%20complex%20group%20objectives)、今後の研究が期待される。

# π₀とπ₀.₅の違いとアーキテクチャ

**共通点と背景**

*  πシリーズは Physical Intelligence が開発する **Vision‑Language‑Action (VLA)** 型ロボット基盤モデルで、多様なロボットやタスクを横断する一般的な行動ポリシーを目指している。どちらもインターネット規模で事前学習した **視覚・言語モデル (VLM)** をベースに、実ロボットの軌跡データから **流れ写像 (flow matching)** 方式で連続アクションを生成する「アクション・エキスパート」を追加する [oai_citation:58‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0#:~:text=Beyond%20training%20on%20many%20different,time%20dexterous%20robot%20control) [oai_citation:59‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf)。
*  π₀とπ₀.₅はどちらも **ロボットの関節トルクやグリッパ操作などの低レベルコマンドを秒間約50 Hzで直接出力**するため、高周波制御が可能である。

### π₀（pi‑zero）の特徴

| 観点           | 内容                                                                                                                                                                                                                                                                                                                                                                     | 出典                       |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------- |
| 事前学習       | 3 BパラメータのVLMを出発点とし、Open X‑Embodimentや自社データ（8種類のロボット）からなる大規模なクロスエンボディメントデータで訓練 [oai_citation:60‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0#:~:text=A%20cross)。                                                                                                                 | Physical Intelligence blog |
| アーキテクチャ | VLMの出力を **連続動作に変換するため、流れ写像を用いたアクション・エキスパート**を追加。これにより離散的な言語トークンしか出力できないVLMを高周波のロボット制御に適合させている [oai_citation:61‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0#:~:text=Beyond%20training%20on%20many%20different,time%20dexterous%20robot%20control)。 | Physical Intelligence blog |
| タスク性能     | 折り畳まれた洗濯物の整列、食器類の片付け、ダンボール箱の組み立て等、複雑なマルチステージ作業を単一モデルで実行できる [oai_citation:62‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0#:~:text=Laundry.%20We%20fine,at%20this%20level%20of%20complexity)。                                                                                | Physical Intelligence blog |
| 評価           | 代替モデル（OpenVLAやOcto）と比較して全タスクで高い成功率を示し、特にVLM事前学習付きの完全モデルが最良の性能を出した [oai_citation:63‡physicalintelligence.company](https://www.physicalintelligence.company/blog/pi0)。                                                                                                                                                 | Physical Intelligence blog |

### π₀.₅（pi‑zero‑point‑five）の特徴とπ₀との違い

| 観点               | π₀.₅（pi‑zero‑point‑five）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | π₀からの主な進化                                                                                                                                                                                                                                                 | 出典                                                                                                  |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| **訓練データ**     | π₀のロボットデータに加え、他ロボットデータ・高レベルサブタスク予測・Webデータなど **異種データの共同学習 (co‑training)** を採用 [oai_citation:64‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=behavior%20that%20is%20appropriate%20to,human%20%E2%80%9Csupervisors%E2%80%9D%20that%20walk%20the)。                                                                                                                                                                                                                                     | 多様なデータソースを統合し、複数の家屋で初見のキッチンや寝室を掃除するなど **オープンワールド一般化**を実現 [oai_citation:65‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=Fig,of%2010%20to%2015%20minutes)。 | π₀は主にロボットデータ＋VLM事前学習であり、π₀.₅は異種データからの知識移転を重視。                     |
| **推論フロー**     | 単一のTransformerモデルで **二段階推論**を行う。①画像と指示から高レベルサブタスク（「皿を取る」など）を予測し [oai_citation:66‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=behavior%20that%20is%20appropriate%20to,human%20%E2%80%9Csupervisors%E2%80%9D%20that%20walk%20the)、②そのサブタスク条件の下でアクション・エキスパートが低レベル行動チャンクを生成 [oai_citation:67‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=The%20%CF%800,represented%20by%20the%20same%20model)。 |                                                                                                                                                                                                                                                                  | π₀はテキスト→行動を直接マッピングする一段推論。π₀.₅は階層的推論により長時間タスクを計画・実行できる。 |
| **モデルの統合**   | サブタスク予測とアクション生成を **同じTransformerが実装**し、マルチモーダル入力をトークン化して順次予測する [oai_citation:68‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf)。ロボット状態もテキストトークンとして入力する。                                                                                                                                                                                                                                                                                                                    | π₀ではサブタスク予測が無く、VLM＋アクションエキスパートのみ。                                                                                                                                                                                                    |
| **アクション表現** | 事前訓練段階では行動を離散トークンとして学習し、後半でπ₀同様に **流れ写像アクション・エキスパート**を追加し連続行動を出力 [oai_citation:69‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=chunks.%20During%20the%20post,the%20web%2C%20and%20training%20then)。                                                                                                                                                                                                                                                                          | π₀と同じく流れ写像を採用するが、訓練レシピが二段階。                                                                                                                                                                                                             |
| **性能**           | 新規のキッチンや寝室で10〜15分に及ぶ掃除タスクを成功させるなど、長時間・複雑な行動を一般化 [oai_citation:70‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=Fig,of%2010%20to%2015%20minutes)。                                                                                                                                                                                                                                                                                                                                            | π₀では一般化能力は限定的で、特定のロボット環境に対するパフォーマンスが中心。                                                                                                                                                                                     |

### π₀/π₀.₅と他のワールドモデルの比較

| モデル                                      | 主なアーキテクチャ/特徴                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 違いのポイント                                                                                                                                                              |
| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **PaLM‑E (Google)**                         | 巨大な言語モデル PaLM を「身体化」し、画像やロボット状態などのセンサーデータを自然言語トークンと同じ空間に埋め込んで入力する **Embodied Language Model** [oai_citation:71‡research.google](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/#:~:text=Today%20we%20introduce%20PaLM,only%20task%20capabilities)。出力は自動回帰的に生成されるテキストであり、低レベル行動は別途言語→行動ポリシーに変換する [oai_citation:72‡research.google](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/#:~:text=tasked%20with%20making%20decisions%20on,level%20robot%20actions)。    | πシリーズはセンサーデータを直接埋め込むのではなく、事前学習済みVLM＋流れ写像で **連続アクションを直接生成**する。PaLM‑Eはタスク計画を文字列として出力し別途ポリシーが必要。 |
| **RT‑2 (Google DeepMind)**                  | PaLI‑XやPaLM‑EなどのVLMを基盤とし、インターネットのテキスト・画像とロボット軌跡データを **同じ形式の言語トークン**に変換して共同学習する。ロボットのアクションも文字列トークンとして表現し、モデルが直接テキスト化された行動を出力し、それをデトークン化してロボット制御に使う [oai_citation:73‡robotics-transformer2.github.io](https://robotics-transformer2.github.io/assets/rt2.pdf#:~:text=we%20propose%20to%20co,action%20models%20%28VLA%29%20and) [oai_citation:74‡robotics-transformer2.github.io](https://robotics-transformer2.github.io/assets/rt2.pdf#:~:text=emergent%20capabilities%20from%20Internet,stage)。 | RT‑2は **離散トークンで行動を表現**するため大規模VLMの知識を活かしやすいが、連続制御にはトークンの分解能が制約となる。πシリーズは流れ写像により連続制御を高周波で出力。     |
| **OpenVLA (Stanford University et al.)**    | 7Bパラメータの **公開VLA**。Llama 2言語モデルとDINOv2＋SigLIP視覚エンコーダーを組み合わせ、約97万回のロボットデモから学習。公開されたチェックポイントをファインチューニングして他ロボットへ適応でき、RT‑2‑Xより小規模ながら性能が高い [oai_citation:75‡arxiv.org](https://arxiv.org/pdf/2406.09246.pdf#:~:text=Large%20policies%20pretrained%20on%20a,We%20further)。                                                                                                                                                                                                                                                         | OpenVLAは離散トークン化による行動出力を採用（PaLM‑E/RT‑2系統）。πシリーズは流れ写像方式で連続アクションを直接生成し、高周波制御に対応。                                     |
| **Helix (Figure AI)**                       | 二段階構造の **System 2 / System 1** アーキテクチャ。System 2は7BパラメータのVLMで7–9 Hzの低周波でシーン理解や言語理解を行い、System 1は80 Mパラメータのクロスアテンション型ビジュオモータポリシーで200 Hzの高周波連続制御を担当 [oai_citation:76‡figure.ai](https://www.figure.ai/news/helix#:~:text=Helix%20is%20a%20first,the%20entire%20humanoid%20upper%20body)。二つのシステムはエンドツーエンド学習され、全身上半身（手首・指まで）を連続制御できる [oai_citation:77‡figure.ai](https://www.figure.ai/news/helix#:~:text=%2A%20Full,torso%2C%20head%2C%20and%20individual%20fingers)。                                 | πシリーズは1つのTransformerとアクションエキスパートで高周波制御を行う。HelixはVLMと低レベルポリシーを分離し、それぞれ異なる時間スケールで動作。                             |
| **SmolVLA (Hugging Face / Sorbonne Univ.)** | VLMの最終層を切り捨てた **小型モデル**に行動エキスパートを付加し、1 GPUで訓練・推論可能。**非同期推論スタック**により認識・行動生成を動作実行と分離し、高制御レートでチャンク生成を行う [oai_citation:78‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=In%20this%20work%2C%20we%20present,a%20range%20of%20both%20simulated) [oai_citation:79‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=a%20single%20GPU%20and%20deployed,its%20compact%20size%2C%20SmolVLA%20achieves)。                                                                                                                             | 小型で省計算を重視しており、連続制御の流れ写像はπシリーズと似るが、コミュニティ収集データやCPUでの実行など低コスト実装が特徴。                                              |
| **FOLIAGE (X. Liu & H. Tang)**              | **物理インテリジェンス用ワールドモデル**。画像・メッシュ・点群を統合するコンテキストエンコーダと、物理行動に条件づけた **物理予測器**から構成され、表面の成長を予測する **Accretive Graph Network** を含む [oai_citation:80‡arxiv.org](https://arxiv.org/abs/2506.03173)。SURF‑BENCHベンチマークでトポロジ認識や物質推定などの能力を評価。                                                                                                                                                                                                                                                                                    | これはロボット行動生成ではなく、物理現象の予測を目的とする世界モデルであり、πシリーズとは用途が異なるが、物理インテリジェンス研究の一例。                                   |
| **Shake‑VLA (H. Khan et al.)**              | カクテル作りのための **音声指示＋視覚＋二腕ロボット**を統合したVLAシステム。視覚モジュールで材料を検出し、音声をテキスト化し、RAGモジュールでレシピを検索して行動生成を行う [oai_citation:81‡arxiv.org](https://arxiv.org/abs/2501.06919#:~:text=%3E%20Abstract%3AThis%20paper%20introduces%20Shake,cocktails%2C%20from%20recipe%20formulation%20to)。                                                                                                                                                                                                                                                                        | 特定タスクに特化したVLAシステム。ロボットが声や環境に応じて二腕で混ぜるなど複雑な操作を行うが、一般的な基盤モデルではない。                                                 |
| **VLA 総合レビュー**                        | IEEE Access 2025 のレビュー論文では、102のVLAモデルや26の基盤データセットを分析し、モデルアーキテクチャやデータ収集の課題、将来の方向性を整理している [oai_citation:82‡vla-survey.github.io](https://vla-survey.github.io/#:~:text=Amid%20growing%20efforts%20to%20leverage,world%20robotic%20systems)。                                                                                                                                                                                                                                                                                                                      | 研究全体の動向を俯瞰する資料として有用。                                                                                                                                    |

# 他のワールドモデル（VLA以外）

* **PaLM‑E**  – 視覚と言語の両方を扱うLLMを「身体化」したモデルで、画像やロボット状態を自然言語トークンと同じ空間に埋め込み、テキストとして行動計画を出力する。センサ情報を直接LLMに注入することで、言語モデルの推論力をロボットタスクに活かす [oai_citation:83‡research.google](https://research.google/blog/palm-e-an-embodied-multimodal-language-model/#:~:text=Today%20we%20introduce%20PaLM,only%20task%20capabilities) [oai_citation:84‡proceedings.mlr.press](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf#:~:text=Large%20language%20models%20excel%20at,for%20multiple%20embodied%20tasks%20in)。
* **RT‑2 (Robotics Transformer 2)**  – PaLI‑X/PaLM‑Eとロボット軌跡データを **同じ言語トークン**形式で学習し、ロボットアクションも文字列で表現する。これにより、ウェブ知識から得た概念を直接ロボット行動へ転移でき、見たことのない物体や命令にも適応する [oai_citation:85‡robotics-transformer2.github.io](https://robotics-transformer2.github.io/assets/rt2.pdf#:~:text=we%20propose%20to%20co,We%20refer%20to) [oai_citation:86‡robotics-transformer2.github.io](https://robotics-transformer2.github.io/assets/rt2.pdf#:~:text=emergent%20capabilities%20from%20Internet,or%20which%20type)。
* **OpenVLA**  – 7Bパラメータの公開VLAで、Llama 2とDINOv2/SigLIPの組合せからなる視覚・言語モデルを用い、Open X‑Embodimentデータセット約97万件で学習する。RT‑2 Xより小規模ながら29タスクで高い成功率を示し、パラメータ効率的な微調整が可能 [oai_citation:87‡arxiv.org](https://arxiv.org/pdf/2406.09246.pdf#:~:text=Large%20policies%20pretrained%20on%20a,OpenVLA%20for%20new%20settings%2C%20with)。
* **Helix**  – Figure AIが発表した一般的ヒューマノイド用VLA。高レベル認識と言語理解を担当するSystem 2（7B VLM）と、200 Hzの高速連続制御を行うSystem 1（80 Mパラメータトランスフォーマ）からなる二段階モデルで、2台のロボットを協調させたり未知の物体を把持する能力を持つ [oai_citation:88‡figure.ai](https://www.figure.ai/news/helix#:~:text=%2A%20Full,torso%2C%20head%2C%20and%20individual%20fingers) [oai_citation:89‡figure.ai](https://www.figure.ai/news/helix#:~:text=Helix%20is%20a%20first,the%20entire%20humanoid%20upper%20body)。
* **SmolVLA**  – 巨大VLAの学習・推論コストを抑えるため、VLMの最終層を削除した小型モデルと流れ写像アクションエキスパートを用い、非同期推論スタックによって認識と行動生成を実行から分離。1つのGPUやCPUで訓練・推論でき、チャンク化した行動生成により応答速度を改善 [oai_citation:90‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=In%20this%20work%2C%20we%20present,a%20range%20of%20both%20simulated).
* **FogROS2‑PLR**  – 大規模モデルをクラウドにオフロードする **クラウドロボティクス** フレームワーク。複数のクラウドサーバとネットワークを利用して **遅延と信頼性のトレードオフ**を最適化し、5GやWi‑Fiの切り替え時でも遅延を最大3.7倍改善 [oai_citation:91‡arxiv.org](https://arxiv.org/html/2410.05562v1)。高計算なVLM/VLAをロボットに搭載せずクラウドから利用する研究の一例である.
* **その他**  – マルチエージェントやスウォームにVLAを適用した事例は現時点でほとんど報告されていない。Helixは2台協調制御を実現したが、数十〜数百台規模のスウォームにVLAを用いた研究は見当たらない。Swarm roboticsでは個々のロボットが軽量な局所制御を行うことが多く、巨大なVLAモデルを中央で推論する方式は通信帯域や同期の制約が大きい。今後、複数ロボットが協調して複雑なタスクを遂行するための **分散型VLA** や **中央集権型のクラウド推論** の研究が期待される.

# 遅延300 msでのクラウドオフロードとスウォーム制御に関する研究

* **リアルタイム制御の課題とRTC**  – VLAモデルは大規模で推論が遅いことが多い。Real‑Time Action Chunking (RTC) は推論と実行のタイミングを非同期化し、現在のチャンクを実行しながら次のチャンクをインペイントすることで **長い推論時間を隠蔽**する手法であり、π₀.₅にそのまま適用できる。このアルゴリズムにより応答遅延を許容範囲内に抑える実験が示されている.
* **SmolVLAの非同期推論**  – 小型モデルでも推論が遅い場合があり、SmolVLAは認識と行動生成を実行から分離する **非同期スタック**を採用し、チャンク生成を先読みすることで高制御レートを実現している [oai_citation:92‡arxiv.org](https://arxiv.org/pdf/2506.01844.pdf#:~:text=a%20single%20GPU%20and%20deployed,its%20compact%20size%2C%20SmolVLA%20achieves)。これはクラウド上のモデルをロボット上の実行スレッドから切り離す際にも有効なアーキテクチャである。
* **FogROS2‑PLRによるクラウドロボティクス**  – クラウドに計算をオフロードする際の遅延と信頼性を分析し、複数の独立サーバとネットワークを用いて締切遅延を確率的に減らす「不可能性三角」を提唱している [oai_citation:93‡arxiv.org](https://arxiv.org/html/2410.05562v1)。これはロボットが外部の大規模モデルを利用する際、300 ms程度の遅延を許容できる場合に有効な手法である.
* **スウォームへの応用**  – 現在のVLA研究は主に単体または少数のロボットを対象としており、数十台規模のロボット群に対してVLA政策を適用した研究は確認できなかった。Swarm roboticsでは個々のロボットが軽量な局所制御を行うことが多く、巨大なVLAモデルを中央で推論する方式は通信帯域や同期の制約が大きい。今後、複数ロボットが協調して複雑なタスクを遂行するための **分散型VLA** や **中央集権型のクラウド推論** の研究が期待される.

# まとめ

1. **π₀はVLMに流れ写像アクションエキスパートを組み込んだ最初の一般ロボット基盤モデル**で、さまざまなロボットとタスクを単一モデルで制御できる。**π₀.₅**はπ₀の上位版で、他ロボットデータやWebデータを共同学習し、サブタスク推論とアクション生成を同一モデルで行う階層的推論を導入することで **未知の家庭環境への一般化性能を大幅に向上**させた [oai_citation:94‡physicalintelligence.company](https://www.physicalintelligence.company/download/pi05.pdf#:~:text=behavior%20that%20is%20appropriate%20to,human%20%E2%80%9Csupervisors%E2%80%9D%20that%20walk%20the).
2. **他のワールドモデルやVLAの潮流**として、PaLM‑Eのようにセンサーデータを直接LLMに埋め込む手法、RT‑2のように行動を言語トークンとして扱う手法、OpenVLAやSmolVLAのようなオープンで軽量化されたモデル、Helixの二系統システムによる高速連続制御、Shake‑VLAのような特定タスク向けシステムがある。FOLIAGEは物理インテリジェンスに焦点を当てた **物理現象の予測モデル**であり、世界モデルの新しい方向性を示している [oai_citation:95‡arxiv.org](https://arxiv.org/abs/2506.03173).
3. **クラウドオフロードと遅延許容**について、RTCやSmolVLAの非同期推論は高遅延環境での連続制御を可能にし、FogROS2‑PLRはネットワーク遅延やサーバ故障の影響を低減するクラウドロボティクス基盤を提供する [oai_citation:96‡arxiv.org](https://arxiv.org/html/2410.05562v1)。これらは大きなモデルをクラウドに置き、小型ロボットが遠隔推論を用いて動作する際に参考となるが、スウォーム全体をVLAで動かす研究は未だ少なく、今後の重要課題である.
